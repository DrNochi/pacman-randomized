\documentclass[10pt, letterpaper]{article}
\usepackage[utf8]{inputenc}

% Set page margins
\usepackage[top=2cm, bottom=4cm, left=2.5cm, right=2cm]{geometry}

% Set up page header & footer
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\headheight 2cm
\lhead{Team Randomized \\}
\chead{\textbf{Artificial Intelligence \\ \Large Pac-Man AI}}
\rhead{CPSC 481--04 \\}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}

% Make list style configurable
\usepackage{enumitem}
\setenumerate[1]{label=\alph*)}
\setenumerate[2]{label=\roman*.}

% Mathematical symbols
\usepackage{amsmath}
\usepackage{amssymb}

% Command for limit with n growing to infinity
\newcommand{\limn}{\lim_{n\to\infty}}

% Add support for code & configure environment
\usepackage{listings}
\lstdefinestyle{mystyle}{
    backgroundcolor=,   
    commentstyle=\it,
    keywordstyle=\bf,
    numberstyle=\em,
    stringstyle=\em,
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,         
    breaklines=false,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% Add & configure TikZ for graphing
\usepackage{tikz}
\usetikzlibrary{arrows,automata}

% Multi column support
\usepackage{multicol}
\setlength{\columnsep}{1cm}

\usepackage{enumitem}
\setlist{nosep}

\usepackage{hyperref}

\usepackage{titlesec}
% \titleformat{\section}{\normalfont\fontsize{12}{15}\bfseries}{\thesection}{1em}{}

\begin{document}
    \begin{center}
        Final Project by Team Randomized:\\
        \emph{Noah Stroehle, Kyle Slater, Ivan Parrales, Victor Duarte}
    \end{center}
    
    \begin{abstract}
        \bf We created a progressive web app (PWA) running a Pac-Man game that can be played as a human or by an Artificial Intelligence. Using different approaches of machine learning \& artificial intelligence, we created a few bots that hook into the "agent interface" of our game. A user/player can either chose to play the game by himself or pick one of the provided bots to watch it play autonomously.
    \end{abstract}

    \section{Goals}
    As a team we decided that we wanted to create a game-playing AI. We chose Pac-Man as it appeared to us as a fun thing to do, as well as not too hard so that it would be doable in our limited time.
    
    Our bot/agent should be capable of playing Pac-Man by taking the place of the human player and being in direct control of Pac-Man. The goal of the agent is to maximize it's score and being able to beat random Pac-Man levels. To accomplish this, we wanted to use some form of (deep) reinforcement learning (e.g. Q-learning / actor critic learning).
    
    As a basis for our project we chose to use an existing (reverse-engineered) Pac-Man game, so our project can focus on creating the actual AI, not recreating the game itself.
    
    \section{Design \& Implementation}
        The project is done in JavaScript using TensorFlow.js for deep learning tasks. While the game is run through the browser, we used Node.js for training to get increased performance.
    
        \subsection{The Game}
            The game is a modularized version of \emph{\href{https://github.com/shaunlebron/pacman}{Shaun Williams' browser-based Pac-Man remake}}. It is designed to be as close to the original Pac-Man as possible.
            
            The "main loop" of the game lives in the file \texttt{executive.js}. It updates the current \texttt{state}, calls the \texttt{renderer} to redraw the canvas and then reschedules itself using the JavaScript function \texttt{requestAnimationFrame()} (falling back to \texttt{setTimeout()} if needed). Using the game's state system (\texttt{states.js}) and the UI / menu system, the game is able to display menus and switch to different user interfaces when the user chose an action. When the player starts a game, the state system is used to change the current state to \texttt{playState}.
            
            In the \texttt{playState} a \texttt{Game} object is used to hold all data relevant to the current gameplay (player, ghosts, map, etc.). Each time the "main loop" is executed the game is advanced one step, player and ghost positions are updated, current ghost behaviour is updated (scatter or chase, recompute targets), pellets are consumed, and so forth.
            
            Each time the player takes a step (basically moves a pixel), which can occur up to 2 times per frame, it is able to change it's direction. During normal play the \texttt{frontend} is able to set \texttt{player.inputDir}, which is then used before each step to update the actual movement direction (if it is a valid direction, not facing a wall). If the user however selected an AI agent to use in the \texttt{AI SETTINGS} screen, \texttt{player.agent} is set and will be used to choose the next action. This allows easy addition of bots by simply implementing the "agent interface" and setting it as \texttt{player.agent}. If the user wants to take over he has the possibility to override the AI's decision, and the AI will only take over again at the next intersection. This makes the AI a real interactive experience.
        
        \subsection{The Bots}
            \subsubsection*{Q learning}
                The first bot we created is bot using classical AI, a simple Q learning implementation with Q table look-ups. Starting out with an empty Q table (empty JS object "\texttt{\{\}}"). At iteration during training a episode is played (an episode ends when Pac-Man dies or clears the level). For each frame the agent then chooses an action using the Q table.
                
                The state is therefore encoded into a feature object containing the direction to the nearest pellet, as well as if a ghost is within one tile. Actions are simply represented by a numerical value (0-3 for up, left, down, right). An action is chosen using it's UCB value calculated from the Q table entries for the current state. Initially we used an $\epsilon$-greedy policy, but it turned out that UCB was able to learn quicker, have better exploration/exploitation trade-off, and converge better on a single action -- $\epsilon$-greedy policy often started to jitter, switching between multiple directions each step --.
                
                $$UCB_{max} = max_{a \in A} \left[ Q(s,a) + \sqrt{\frac{2\ ln N_s}{n_{s,a}}}\ \right] \Rightarrow a^*$$
                
                At the end of each episode, the \texttt{replayBuffer} is used to update the Q table entries accoring to a variant of the Bellman equation suitable for use with UCB:
                
                $$R_{tot,i} = r_i + 0.8 * R_{tot,i+1}$$
                $$Q(s_i,a_i)' = Q(s_i,a_i) + R_{tot,i}$$
                $$n_{s_i,a_i}' = n_{s_i,a_i} + 1 \text{ and } N_{s_i}' = N_{s_i} + 1$$
                
                \vspace{1em}\noindent
                \textbf{Results:} Bot navigates maze without any problems, but it is mostly not able to clear the level as it often gets cornered by ghosts, and sometime runs into them. This is due the limited amount of data the state encodes. Ghosts are only registered by the agent when they are on an adjacent tile. Also the bot has no time like perception, so there is no way of registering changes over time, like movement direction. Average score: 1000-1500, Best score: 5800
                
            \subsubsection*{Q learning with human designed Q table}
                This agent is the same as the basic Q learning agent, but the Q table is not initialized empty, but specific values designed by us to result in the best performance. This was possible due to the simple state encoding, which drastically limits the size of the Q table.
                
                \vspace{1em}\noindent
                \textbf{Results:} Bot is now able to clear levels ($\approx 15-20\%$), but still has trouble with ghosts entering a tile in the same frame as Pac-Man. Average score: around 2000, Best score: 6340
                
            \subsubsection*{Policy network}
            \subsubsection*{Deep Q learning}
            \subsubsection*{Deep Q learning (CNN)}
            \subsubsection*{Deep AC learning}
        
    \section{Problems (\& Solutions)}
        \subsection*{Modularization \& API}
            We started out using the Pac-Man remake as is, trying to create a bot for it. It turned out this wasn't the optimal approach, as that remake project was never meant to be used for AI purposes. It could not be used in Node.js, which we wanted to use for training, and it did not expose a way of easily integrating and switching between different AI agents.
            
            \vspace{1em}\noindent
            \textbf{Solution:} Rewrite the game ourselves in a modular fashion that is extensible, works well with Node.js, and exposes a way of easily adding new AI agents.
            
        \subsection*{Training time}
            As it turns out, training a Pac-Man bot using (deep) reinforcement learning is computationally very expensive and takes A LOT of time we sadly didn't have. \emph{\href{https://towardsdatascience.com/advanced-dqns-playing-pac-man-with-deep-reinforcement-learning-3ffbd99e0814}{Online sources}} mention that the standard approach in DQN literature is a (final) training time of 200 million frames (roughly 1 million games \& two weeks of time). But to get at least some good results 10 million frames (approximately 24 hours) should be sufficient.
            
            \vspace{1em}\noindent
            \textbf{Solution:} Just train for longer, or optimize the hyper-parameters to speed up training.
    \newpage
        
    \section{Features}
        \subsection{The Game}
            \begin{itemize}
                \item Pac-Man \& all 4 ghosts
                \item Pellets \& power pellets
                \item Different ghost behaviours
                \item Random map generation
                \item Debug mode: Invincible Pac-Man, visible ghost targets, grid lines, additional logging
                \item As close as possible to the original arcade game (timings, speeds, scores)
            \end{itemize}
            
        \subsection{The Bots}
            \begin{itemize}
                \item Q-learning ("\texttt{Q LEARNING}")
                \item Q-learning with human designed Q table ("\texttt{Q LEARNING (HUMAN)}")
                % \item Deep Q learning
                % \item Deep Q learning CNN
            \end{itemize}
            
        \subsection{Web App}
            \begin{itemize}
                \item Responsive \& mobile capable (touch controls)
                \item Progressive Web App:
                    \begin{itemize}
                        \item Runnable on any web-capable device
                        \item Can be downloaded and installed on your phones homescreen (Android)
                        \item Immersive fullscreen user experience after install \\
                              $\rightarrow$ feels like a normal app
                        \item Offline mode
                    \end{itemize}
                \item Build system using npm with rollup \& terser to pack and minify the JavaScript code for "production"/distribution
            \end{itemize}
            
    \section{Conclusion}
    
    \section{Future Directions}
        In future "iterations" it would be possible to further improve the bots performance by tackling the aforementioned problems. Maybe other algorithms / approaches could be tried, e.g. similar to Google DeepMinds Atari AI. Other possibilities include improving on the game itself (fruits, elroy mode, Ms. Pac-Man, Crazy Otto, Cookie Man, audio) 
        
        \vspace{1em}\noindent
        Additionally one could investigate adding additional game modes:
        \begin{itemize}
            \item Human vs Bot: side by side, playing simultaneously trying to reach a higher score than the opponent
            
            \item Human vs Bot: playing as a ghost you try to catch the bot-controlled Pac-Man
            
            \item Bot vs Bot: training two adversarial bots (one for Pac-Man, one for the ghosts)
        \end{itemize}
    
\end{document}
